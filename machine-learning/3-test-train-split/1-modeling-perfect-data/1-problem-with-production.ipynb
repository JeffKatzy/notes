{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What could go wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data scientists, we are constantly making mistakes.  Our task is to predict the future or to accurately explain the present world, and both of these tasks are hard.  \n",
    "\n",
    "We do our best by training our machine learning model to find the parameters that best predict future outcomes.  And while this technique is the best that we've got, it still has problems.  In the sections that follow, we'll explore some of the problems that occur.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us count the ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, the difference between what our model predicts and what we observe is our error.  And as we'll see, there are three different sources of this error.\n",
    "\n",
    "#### 1. Irreducible Error\n",
    "Irreducible error means that our future outcomes will have a degree of randomness to them.  Randomness in future outcomes means that our machine learning model cannot make predictions with 100 percent accuracy.  This is called irreducible error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Variance\n",
    "Variance occurs because *the data we train our models with also has a degree of randomness in it*.  The randomness in our training data affect the parameters that our machine learning algorithm arrives at.  You can imagine that if you trained the model many times, it would be fed different variations of data, and thus arrive at different outcomes.  This variation in our parameters based on randomness is called variance. \n",
    "\n",
    "But note, with variance while the randomness may affect each of our models, and make each of them wrong.  If we averaged our parameters, this variation should cancel out.  Thus we could average our parameters over many models to approach the true parameters of the underlying model (so long as there are no other errors). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Bias\n",
    "\n",
    "Bias occurs when we do not have enough parameters to approach the true underlying model.  We are simply not including all of the influences that determine our outcomes.  Unlike the other sources of error, bias doesn't occur because of randomness, but rather because our model isn't fed the proper data to even see what causes all of the variation in our outcomes.  So unlike with variance, even if we were to train our machine learning algorithm many times, we would still not have an accurate model, because our model is not seeing all of the influences to our outcomes.  This is bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw that there are three sources of error when we train a machine learning model: irreducible error, variance, and bias.  Irreducible error is simply the error that comes from our machine learning model's inability to predict the randomness expressed in future outcomes.  \n",
    "\n",
    "Variance is the recognition that because our machine learning model trains on data that also has a degree of randomness, the randomness shows up in the parameters of our model.  If we trained our model many times, we would see variations of the parameters.  However, if we averaged our the parameters we see from these multiple trainings (with each training fed different data), we would expect the randomness to cancel out, and for our model approach the true underlying model.  \n",
    "\n",
    "Finally, bias occurs from our model not training on the features contribute to the outcomes.  Thus, even if we were to train our model on these same features multiple times, the model still would not be able to approach the true underlying model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
